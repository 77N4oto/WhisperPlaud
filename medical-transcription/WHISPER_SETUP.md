# Whisperçµ±åˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€faster-whisperã‚’ä½¿ã£ãŸå®Ÿéš›ã®éŸ³å£°æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“‹ å‰ææ¡ä»¶

- **Python 3.11+**
- **ä»®æƒ³ç’°å¢ƒ**: `whisper-pyannote-env`ï¼ˆæ—¢å­˜ï¼‰
- **GPUï¼ˆæ¨å¥¨ï¼‰**: NVIDIA CUDAå¯¾å¿œGPUï¼ˆRTX 3060 12GBç­‰ï¼‰
- **ãƒ¡ãƒ¢ãƒª**: æœ€ä½8GB RAMï¼ˆlarge-v3ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚ã¯16GBæ¨å¥¨ï¼‰

---

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### 1. Pythonä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ

```powershell
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰
.\whisper-env\Scripts\Activate.ps1
```

### 2. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

#### CUDAå¯¾å¿œGPUç’°å¢ƒï¼ˆæ¨å¥¨ï¼‰

```powershell
# PyTorch (CUDA 11.8å¯¾å¿œ)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Whisperã¨éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install -r medical-transcription/requirements-worker.txt
```

#### CPUç’°å¢ƒï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

```powershell
# PyTorch (CPUç‰ˆ)
pip install torch torchvision torchaudio

# Whisperã¨éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install -r medical-transcription/requirements-worker.txt
```

### 3. GPUå¯¾å¿œã®ç¢ºèª

```powershell
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ï¼ˆGPUç’°å¢ƒï¼‰**:
```
CUDA available: True
GPU: NVIDIA GeForce RTX 3060
```

---

## ğŸ¯ Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®é¸æŠ

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | VRAMä½¿ç”¨é‡ | é€Ÿåº¦ | ç²¾åº¦ | æ¨å¥¨ç”¨é€” |
|--------|-------------|-----------|------|------|---------|
| `tiny` | 39M | ~1GB | æœ€é€Ÿ | ä½ | ãƒ†ã‚¹ãƒˆç”¨ |
| `base` | 74M | ~1GB | é«˜é€Ÿ | ä¸­ | è»½é‡å‡¦ç† |
| `small` | 244M | ~2GB | ä¸­é€Ÿ | ä¸­ | ãƒãƒ©ãƒ³ã‚¹å‹ |
| `medium` | 769M | ~5GB | ä¸­é€Ÿ | é«˜ | é«˜ç²¾åº¦ |
| `large-v2` | 1550M | ~10GB | ä½é€Ÿ | æœ€é«˜ | æœ€é«˜ç²¾åº¦ |
| `large-v3` | 1550M | ~10GB | ä½é€Ÿ | æœ€é«˜ | **åŒ»ç™‚ç”¨æ¨å¥¨** |

### ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®è¨­å®šæ–¹æ³•

`.env.local` ã«ä»¥ä¸‹ã‚’è¿½åŠ ï¼š

```bash
# Whisper model size: tiny, base, small, medium, large-v2, large-v3
WHISPER_MODEL_SIZE=large-v3
```

**æ¨å¥¨è¨­å®š**:
- **æœ¬ç•ªç’°å¢ƒï¼ˆGPUï¼‰**: `large-v3`ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
- **é–‹ç™ºç’°å¢ƒï¼ˆGPUï¼‰**: `medium`ï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰
- **ãƒ†ã‚¹ãƒˆç’°å¢ƒï¼ˆCPUï¼‰**: `base`ï¼ˆè»½é‡ï¼‰

---

## ğŸ§ª ãƒ†ã‚¹ãƒˆæ–¹æ³•

### 1. Whisper Processorã®å˜ä½“ãƒ†ã‚¹ãƒˆ

```powershell
# ãƒ†ã‚¹ãƒˆç”¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ï¼ˆMP3, WAV, M4Aç­‰ï¼‰
# ä¾‹: test_audio.mp3

cd medical-transcription
python src/workers/whisper_processor.py path/to/test_audio.mp3
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
Testing Whisper processor with: test_audio.mp3
Initializing Whisper model: large-v3
Device: cuda, Compute type: float16
âœ… Model loaded successfully on cuda

Model info:
{
  "model_size": "large-v3",
  "device": "cuda",
  "compute_type": "float16",
  "loaded": true
}

Transcribing...
================================================================================
TRANSCRIPTION RESULT
================================================================================

Language: ja (prob: 98.45%)
Duration: 45.23s
Confidence: -0.35

Full text:
ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ç³–å°¿ç—…ã®æ²»ç™‚ã«ã¤ã„ã¦ãŠè©±ã—ã—ã¾ã™ã€‚...

Segments: 15
```

### 2. ãƒ¯ãƒ¼ã‚«ãƒ¼å…¨ä½“ã®çµ±åˆãƒ†ã‚¹ãƒˆ

#### ã‚¹ãƒ†ãƒƒãƒ—1: Dockerèµ·å‹•

```powershell
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰
docker compose up -d redis minio
```

#### ã‚¹ãƒ†ãƒƒãƒ—2: Next.jsé–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•

```powershell
# Terminal 1
cd medical-transcription
npm run dev
```

#### ã‚¹ãƒ†ãƒƒãƒ—3: Pythonãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•

```powershell
# Terminal 2
.\whisper-env\Scripts\Activate.ps1
python medical-transcription/src/workers/transcription_worker.py
```

**æœŸå¾…ã•ã‚Œã‚‹ãƒ­ã‚°å‡ºåŠ›**:
```
INFO - Transcription worker initialized
INFO - Redis: redis://localhost:6379
INFO - S3: http://localhost:9000
INFO - Bucket: medical-transcription
INFO - Whisper model: large-v3
INFO - Worker started. Subscribing to 'job:new' channel...
INFO - Waiting for jobs...
```

#### ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨æ–‡å­—èµ·ã“ã—

1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:3000/login` ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ãƒ­ã‚°ã‚¤ãƒ³: `admin` / `test123`
3. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆMP3/WAV/M4Aï¼‰ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—
4. å‡¦ç†ã®é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç¢ºèª

**æœŸå¾…ã•ã‚Œã‚‹å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
```
2% - Loading AI model...
5% - Downloading audio file...
10% - Transcribing audio with Whisper...
15-70% - Processing audio segments... (X)
75% - Applying medical term corrections...
90% - Saving transcript...
100% - Completed
```

---

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ1: `ModuleNotFoundError: No module named 'faster_whisper'`

**åŸå› **: ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„

**è§£æ±ºæ–¹æ³•**:
```powershell
.\whisper-env\Scripts\Activate.ps1
pip install -r medical-transcription/requirements-worker.txt
```

---

### å•é¡Œ2: `CUDA out of memory`

**åŸå› **: GPUãƒ¡ãƒ¢ãƒªä¸è¶³

**è§£æ±ºæ–¹æ³•**:
1. `.env.local` ã§ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹:
   ```bash
   WHISPER_MODEL_SIZE=medium  # ã¾ãŸã¯ base
   ```

2. ã¾ãŸã¯ã€CPUç’°å¢ƒã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆè‡ªå‹•ï¼‰

---

### å•é¡Œ3: æ–‡å­—èµ·ã“ã—ãŒæ¥µç«¯ã«é…ã„ï¼ˆCPUç’°å¢ƒï¼‰

**åŸå› **: CPUã§ã®å‡¦ç†ã¯éå¸¸ã«é…ã„ï¼ˆGPUæ¯”ã§10-20å€ï¼‰

**è§£æ±ºæ–¹æ³•**:
1. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹: `base` ã¾ãŸã¯ `tiny`
2. GPUç’°å¢ƒã‚’ç”¨æ„ã™ã‚‹ï¼ˆæ¨å¥¨ï¼‰
3. ã‚ˆã‚ŠçŸ­ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ†ã‚¹ãƒˆã™ã‚‹

---

### å•é¡Œ4: æ—¥æœ¬èªãŒæ­£ã—ãèªè­˜ã•ã‚Œãªã„

**åŸå› **: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹ã€ã¾ãŸã¯è¨€èªè¨­å®šãŒé–“é•ã£ã¦ã„ã‚‹

**è§£æ±ºæ–¹æ³•**:
1. `medium` ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
2. `whisper_processor.py` ã® `language='ja'` ã‚’ç¢ºèª

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®å®‰

### GPUç’°å¢ƒï¼ˆRTX 3060 12GBï¼‰

| ãƒ¢ãƒ‡ãƒ« | 1åˆ†éŸ³å£°ã®å‡¦ç†æ™‚é–“ | VRAMä½¿ç”¨é‡ |
|--------|------------------|-----------|
| `base` | ~5ç§’ | ~1.5GB |
| `medium` | ~15ç§’ | ~5GB |
| `large-v3` | ~30ç§’ | ~10GB |

### CPUç’°å¢ƒï¼ˆi7-12700ï¼‰

| ãƒ¢ãƒ‡ãƒ« | 1åˆ†éŸ³å£°ã®å‡¦ç†æ™‚é–“ |
|--------|------------------|
| `tiny` | ~1åˆ† |
| `base` | ~2åˆ† |
| `small` | ~5åˆ† |
| `medium` | ~15åˆ† |

---

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

Whisperçµ±åˆãŒå®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ã§ãã¾ã™ï¼š

1. **è©±è€…åˆ†é›¢ï¼ˆpyannoteï¼‰** - è¤‡æ•°è©±è€…ã®è‡ªå‹•è­˜åˆ¥
2. **è¦ç´„ç”Ÿæˆï¼ˆOllamaï¼‰** - çŸ­/ä¸­/é•·ã®3ç¨®é¡ã®è¦ç´„
3. **å…¨æ–‡æ¤œç´¢ï¼ˆSQLite FTS5ï¼‰** - åŸæ–‡+ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
4. **UIã®æ”¹å–„** - ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã€è©±è€…ãƒ©ãƒ™ãƒ«ç·¨é›†

---

## ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å‚è€ƒ

### `.env.local` å®Œå…¨ç‰ˆ

```bash
# èªè¨¼
AUTH_USERNAME=admin
AUTH_PASSWORD_HASH=\$2b\$10\$gKNA0zkRnhgIad26tgVGK.7dbligjYfILZRerAB6NFUwJrHzb1Y6i
JWT_SECRET=dev-secret-key-change-in-production

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
DATABASE_URL=file:./prisma/transcription.db

# Redis
REDIS_URL=redis://localhost:6379

# MinIO (S3äº’æ›)
S3_ENDPOINT=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_BUCKET=medical-transcription
S3_REGION=us-east-1

# Whisperè¨­å®š
WHISPER_MODEL_SIZE=large-v3  # tiny, base, small, medium, large-v2, large-v3
```

---

## ğŸ”— å‚è€ƒè³‡æ–™

- **faster-whisper**: https://github.com/SYSTRAN/faster-whisper
- **Whisperå…¬å¼**: https://github.com/openai/whisper
- **PyTorch CUDA**: https://pytorch.org/get-started/locally/
- **åŒ»ç™‚ç”¨èªè¾æ›¸**: `medical-transcription/medical_dictionary.json`

---

**ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†å¾Œã¯ã€å®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã§å‹•ä½œç¢ºèªã—ã¦ãã ã•ã„ï¼** ğŸ‰

